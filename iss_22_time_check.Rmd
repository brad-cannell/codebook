---
title: "Time test codebook 2"
---

See how much processing time it adds when we check for variables that shouldn't have summary stats run.

```{r}
library(dplyr)
library(readr)
devtools::load_all()
```

```{r}
combined_participant_data <- read_rds("/Users/bradcannell/Library/CloudStorage/OneDrive-TheUniversityofTexasHealthScienceCenteratHouston/01_research/L2C Teams/Participant Data/R data/combined_participant_data.rds")
```

## Codebook

```{r}
test <- combined_participant_data %>% 
  select(1:50)
```

```{r}
start <- lubridate::now()
test_cb <- codebook(test)
end <- lubridate::now()
```

```{r}
end - start # Time difference of 14.25604 secs for 50 cols
```

```{r}
print(test_cb, "test.docx")
```

## Codebook2

The simplest thing is to start by just checking against the no_summary_stats every time. See how big of a deal that is before you do a bunch of extra coding. If it is too slow, here are some other ideas to consider:
* First test to see if any vars were passed to no_summary_stats.
* Calculate the stats for all vars, then drop them where col_name is in no_summary_stats?
* What if we break the column attributes tables and the summary stats tables into two separate loops?

```{r}
start <- lubridate::now()
test_cb2 <- codebook2(test)
end <- lubridate::now()
```

```{r}
end - start # Time difference of 15.32204 secs for 50 cols
```

```{r}
print(test_cb2, "test2.docx")
```

```{r}
start <- lubridate::now()
test_cb3 <- codebook2(test, no_summary_stats = c("ml_name_first", "ml_name_middle_init"))
end <- lubridate::now()
```

```{r}
end - start # Time difference of 15.32204 secs for 50 cols
```

```{r}
print(test_cb3, "test3.docx")
```

It seems like it's working and the new code is actually faster than the old code. I think this solution is fine for now. 
