---
title: "Codebook is Slow"
---

# Overview

Working on issue #17. Codebook is slow.

I think I found the best clue for fixing so far: https://ardata-fr.github.io/officeverse/officer-for-word.html#external-documents



```{r}
library(dplyr)
library(codebookr)
library(microbenchmark)
library(profvis)
library(ggplot2)
```

```{r}
devtools::load_all()
```


```{r}
data(study)
data_stata <- haven::read_dta("inst/extdata/study.dta")
```

How long does it take to run regular data?

```{r}
microbenchmark(
  codebook(study),
  times = 10L
) # 2-3 seconds each run.
```

How long does it to run on Stata data?

```{r}
microbenchmark(
  codebook(data_stata),
  times = 10L
) # 2-3 seconds each run
```

So, that doesn't seem to make a huge difference. 

What are the slow parts?

```{r}
profvis(codebook(study))
```

The Flextable stuff is the slowest part. I'm not sure if I can speed that up or not. 

```{r}
profvis(codebook(data_stata))
```

Flextable stuff for this one too. 

Can I do the Flextable stuff at once outside of a loop? Will that make any difference?


Do more rows slow it down?

```{r}
df_short <- tibble(x = rnorm(100)) # 100 rows
df_medium <- tibble(x = rnorm(10000)) # 10,000 rows
df_long <- tibble(x = rnorm(10000000)) # 10,000,000 rows
```

```{r}
microbenchmark(
  codebook(df_short),  # Mean = 347	milliseconds
  codebook(df_medium), # Mean = 1589 milliseconds
  codebook(df_long),   # Mean = 4212	milliseconds
  times = 10L
)
```

So, adding more observations slows it down.
100 to 10,000 = 4 times as long
100 to 10,000,000 = 12 times as long

Do more columns slow it down?

```{r}
# Keep the first 100 rows of df_long only
df_medium <- df_medium[1:100,]
# Make 100 column names from combinations of letters
set.seed(123)
cols <- unique(paste0(sample(letters, 100, TRUE), sample(letters, 100, TRUE), sample(letters, 100, TRUE)))
for (col in cols) {
  df_medium[[col]] <- rnorm(100)
}
```

```{r}
microbenchmark(
  codebook(df_short),  # Mean = 300	milliseconds
  codebook(df_medium), # Mean = 52776	milliseconds (52 seconds)
  times = 1L
)
```

So, adding more columns slows it down A LOT!
1 to 100 = 175 times as long!

What parts of the code take the longest to run?

```{r}
profvis(codebook(df_short))
```

The flextable parts take the longest (i.e., body_add_flextable and regular_table).

```{r}
profvis(codebook(df_medium))
```

The flextable parts take the longest (i.e., body_add_flextable, body_add_par, and regular_table).

```{r}
profvis(codebook(df_long))
```

unique.default and cb_add_summary stats take the longest.

There isn't a way for me to change the internals of the flextable functions, but I do wonder if me applying them in a different way would speed things up?


# L2C data

Other things to potentially try
* HTML
* Adding a progress bar

The Link2Care data currently has 1,285 rows and 1,205 variables. I let codebook run all night and it still didn't finish. When I click the stop sign in R Studio to stop running the code, I get a message that says "no loop for break/next, jumping to top level". After doing some searching, it looks like that message is related to using the `next()` function or the `break()` function outside of a for loop. I searched my code and I don't use either of those functions anywhere. So, I think that message might be related to the use of the stop sign button in RStudio -- although, I'm not entirely sure.

The nest thing I'm trying is adding `print(paste("Done with", col_nms[[i]]))` inside the `codebook()` function itself. I'm going to see if this tells me where there is a specific problem. 

```{r}
path <- "/Users/bradcannell/Library/CloudStorage/OneDrive-TheUniversityofTexasHealthScienceCenteratHouston/01_research/L2C Teams/Participant Data/R data/combined_participant_data.rds"
```

```{r}
combined_participant_data <- readr::read_rds(path)
```

```{r}
test <- combined_participant_data %>% 
  select(1:50)
```

```{r}
start <- lubridate::now()
test_cb <- codebook(test)
end <- lubridate::now()
```

```{r}
# test_cb
```

```{r}
title <- paste("Total time for ", ncol(test), " columns = ", round(end - start, 2), " seconds")
ggplot(test_cb, aes(x = var_num, y = seconds)) +
  geom_line() +
  ggtitle(title)
```

So, each iteration of the loop progressively gets slower and slower. This is probably really important. 

Now, let's try with 100 columns

```{r}
test <- combined_participant_data %>% 
  select(1:100)
```

```{r}
start <- lubridate::now()
test_cb <- codebook(test)
end <- lubridate::now()
```

```{r}
title <- paste("Total time for ", ncol(test), " columns = ", round(end - start, 2), " minutes")
ggplot(test_cb, aes(x = var_num, y = seconds)) +
  geom_line() +
  ggtitle(title)
```


# Rebuilding the codebook function

We will break the loop up into smaller components to see if that solves the problem or at least gives us more details about what the problem is. The flextable and officer stuff is the slowest part of the process. I wonder if we can't speed up the overall time by storing calculated tables to lists and then calling the flextable functions on the lists in a vectorized way?

```{r}
combined_participant_data <- readr::read_rds("/Users/bradcannell/Library/CloudStorage/OneDrive-TheUniversityofTexasHealthScienceCenteratHouston/01_research/L2C Teams/Participant Data/R data/combined_participant_data.rds")
```

```{r}
test_50 <- combined_participant_data %>% 
  select(1:50)
```

```{r}
test_100 <- combined_participant_data %>% 
  select(1:100)
```

```{r}
# Create vector of column names
col_nms_50 <- names(test_50)
```

```{r}
# Create vector of column names
col_nms_100 <- names(test_100)
```

## Get column attributes for each variable

### Time difference with and without flextable code

```{r}
get_col_att_w_flextable <- function() {
  attributes_list <- list()
  for (i in seq_along(col_nms_100)) {
      # Get column attributes
      attributes_list[[i]] <- test_100 %>%
        cb_get_col_attributes(col_nms_100[[i]], keep_blank_attributes = FALSE) %>%
        flextable::regulartable() %>%
        cb_theme_col_attr()
  }
}

microbenchmark(get_col_att_w_flextable(), times = 10L, unit = "seconds")
```

```{r}
# flextable::regulartable() vs flextable::flextable()
get_col_att_w_flextable <- function() {
  attributes_list <- list()
  for (i in seq_along(col_nms_100)) {
      # Get column attributes
      attributes_list[[i]] <- test_100 %>%
        cb_get_col_attributes(col_nms_100[[i]], keep_blank_attributes = FALSE) %>%
        flextable::flextable() %>%
        cb_theme_col_attr()
  }
}

microbenchmark(get_col_att_w_flextable(), times = 10L, unit = "seconds")
```

With 50 columns flextable::regulartable() vs flextable::flextable() doesn't appear to make much of a difference (mean = ~ 2.5). 
With 100 columns flextable::regulartable() vs flextable::flextable() doesn't appear to make much of a difference (mean = ~ 4.8). 

```{r}
get_col_att_no_flextable <- function() {
  attributes_list <- list()
  for (i in seq_along(col_nms_50)) {
      # Get column attributes
      attributes_list[[i]] <- test_50 %>%
        cb_get_col_attributes(col_nms_50[[i]], keep_blank_attributes = FALSE)
  }
}

microbenchmark(get_col_att_no_flextable(), times = 10L, unit = "seconds")
```

The code takes than half the time with out flextable (~0.8 with 50 cols). Of course, we still need to convert each data frame into a flextable somewhere. So, what if we make it a separate process?

```{r}
attributes_list <- list()
for (i in seq_along(col_nms_50)) {
    # Get column attributes
    attributes_list[[i]] <- test_50 %>%
      cb_get_col_attributes(col_nms_50[[i]], keep_blank_attributes = FALSE)
}
```

```{r}
flextable::regulartable(attributes_list)
```

Passing the list of data frames to flextable doesn't work. So, we'd still have put the code into some kind of loop. I can't imagine that two separate loops to do the same thing is faster than one. 

```{r}
get_col_att_w_flextable <- function() {
  attributes_list <- list()
  for (i in seq_along(col_nms_50)) {
      # Get column attributes
      attributes_list[[i]] <- test_50 %>%
        cb_get_col_attributes(col_nms_50[[i]], keep_blank_attributes = FALSE) %>%
        flextable::regulartable() %>%
        cb_theme_col_attr()
  }
}

microbenchmark(get_col_att_w_flextable(), times = 10L, unit = "seconds")
```

```{r}
get_col_att_w_flextable_2_loops <- function() {
  attributes_list <- list()
  for (i in seq_along(col_nms_50)) {
      # Get column attributes
      attributes_list[[i]] <- test_50 %>%
        cb_get_col_attributes(col_nms_50[[i]], keep_blank_attributes = FALSE)
  }
  for (i in seq_along(attributes_list)) {
      # Get column attributes
      attributes_list[[i]] <- flextable::flextable(attributes_list[[i]]) %>%
        cb_theme_col_attr()
  }
}

microbenchmark(get_col_att_w_flextable_2_loops(), times = 10L, unit = "seconds")
```

Breaking it up into two parts results in about the same speed as doing it all in one loop (mean = 2.3 and 2.3)

### for loop vs purrr

```{r}
get_col_att_w_flextable <- function() {
  attributes_list <- list()
  for (i in seq_along(col_nms_50)) {
      # Get column attributes
      attributes_list[[i]] <- test_50 %>%
        cb_get_col_attributes(col_nms_50[[i]], keep_blank_attributes = FALSE) %>%
        flextable::regulartable() %>%
        cb_theme_col_attr()
  }
}

microbenchmark(get_col_att_w_flextable(), times = 10L, unit = "seconds")
```

```{r}
get_col_att_w_purrr <- function() {
  attributes_list <- purrr::map(
    .x = col_nms_50,
    .f = ~ cb_get_col_attributes(test_50, .x, keep_blank_attributes = FALSE) %>% 
      flextable::regulartable() %>%
        cb_theme_col_attr()
  )
}

microbenchmark(get_col_att_w_purrr(), times = 10L, unit = "seconds")
```

No difference between for loop and purrr (mean = 2.4 and 2.4)

Perhaps we need to just extract the adding the flextables to the rdocx object part into a separate step. I think I have to make these changes directly inside of the codebook function.

```{r}
start <- lubridate::now()
test_cb <- codebook(test_50)
end <- lubridate::now()
```

```{r}
title <- paste("Total time for ", ncol(test_50), " columns = ", round(end - start, 2), " seconds")
ggplot(test_cb, aes(x = var_num, y = seconds)) +
  geom_line() +
  ggtitle(title)
```

Make separate:
1. List of attribute flextables
2. List of summary tables
3. Add attribute ad summary tables to docx object.

```{r}
start <- lubridate::now()
test_cb <- codebook(test_50)
end <- lubridate::now()
```

```{r}
title <- paste("Total time for ", ncol(test_50), " columns = ", round(end - start, 2), " seconds")
ggplot(test_cb, aes(x = var_num, y = seconds)) +
  geom_line() +
  ggtitle(title)
```

Ok, so how do we iteratively add flextables to an rdocx object?

```{r}
test_rdocx <- officer::read_docx() %>%
    officer::cursor_begin()
```

```{r}
test_rdocx <- test_rdocx %>%
    cb_add_section_header("Column Attributes:")
```

```{r}
summary_stats_list <- codebook(test_50)
```

```{r}
# Iteratively add summary_stats_list to test_rdocx
# test_rdocx <- test_rdocx %>%
#     flextable::body_add_flextable(summary_stats_list[[1]])

test_rdocx <- purrr::map(
  .x = summary_stats_list,
  .f = ~ flextable::body_add_flextable(test_rdocx, .x)
)
```

```{r}
print(test_rdocx, "test.docx")
```


# External documents

Testing out the mini external documents solution found here: https://ardata-fr.github.io/officeverse/officer-for-word.html#external-documents

Inserting a document of course allows you to integrate a previously-created Word document into another document. This can be useful when certain parts of a document need to be written manually but automatically integrated into a final document. The document to be inserted must be in docx format.

This can be done by using function `body_add_docx()`.

This can be advantageous when you are generating huge documents and the generation is getting slower and slower.

It is necessary to generate smaller documents and to design a main script that inserts the different documents into a main Word document.

## codebook 1

Running the established `codebook()` code first as a reference.

```{r}
combined_participant_data <- readr::read_rds("/Users/bradcannell/Library/CloudStorage/OneDrive-TheUniversityofTexasHealthScienceCenteratHouston/01_research/L2C Teams/Participant Data/R data/combined_participant_data.rds")
```

```{r}
test_50 <- combined_participant_data %>% 
  select(1:50)
```

```{r}
test_100 <- combined_participant_data %>% 
  select(1:100)
```

```{r}
start <- lubridate::now()
test_cb <- codebook(test_50)
end <- lubridate::now()
```

```{r}
title <- paste("Total time for ", ncol(test_50), " columns = ", round(end - start, 2), " seconds")
ggplot(test_cb, aes(x = var_num, y = seconds)) +
  geom_line() +
  ggtitle(title)
```

## codebook2

Using the smaller documents approach described at the top of this section.

```{r}
start <- lubridate::now()
test_cb <- codebook2(test_50)
end <- lubridate::now()
```

```{r}
# title <- paste("Total time for ", ncol(test_50), " columns = ", round(end - start, 2), " seconds")
# ggplot(test_cb, aes(x = var_num, y = seconds)) +
#   geom_line() +
#   ggtitle(title)
```

```{r}
# print(test_cb, "test.docx")
```

```{r}
end - start # 14.7 secs
```

```{r}
start <- lubridate::now()
test_cb <- codebook2(test_100)
end <- lubridate::now()
```

```{r}
end - start # 27.9 secs
```

```{r}
print(test_cb, "test.docx")
```


# Testing new codebook code

After moving the codebook2 code over to codebook. 

```{r}
start <- lubridate::now()
test_cb <- codebook(test_50)
end <- lubridate::now()
```

```{r}
print(test_cb, "test.docx")
```

```{r}
end - start # 14.7 secs
```




